{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation Maximization \n",
    "\n",
    "Expectation Maximization (EM) is an algorithm you can use to estimate the maximum likelihood for the value of latent (hidden) parameters. Suppose, for example, that your friend possesses a bag of five unfair coins, but you don't know the \"weights\" of the coins--i.e., each coin's probability of coming up heads. Your friend offers to shake the bag, pick a coin out of the bag, flip it 100 times, then put it back in the bag. He goes through this cycle this 100 times. The visible parameters are the 100 sessions of heads or tails; the latent parameters are the \"weights\" of the five coins. How can you estimate the latent parameters? EM provides a useful method.\n",
    "\n",
    "### Simulation scenario\n",
    "\n",
    "In a simulation deviously devised by Professor David Crandall, a candy company has 5 different probabilities of inserting cherry candies into bags of cherry and lime candies. You, the student, do not know what these probabilities are. However, you are allowed to enter a dark truck and randomly select 100 bags of candy. You may then draw 100 candies from each bag. Your goal is to estimate the 5 cherry probabilities--the latent parameters--based on your observations. You must also provide a maximum likelihood estimate of which of the 5 probability distributions was used by the manufacturer for each of the 100 bags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: The setup code was written by Professor Crandall. I did insert one optimization by converting a list to a numpy array, though.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "#####\n",
    "# You shouldn't have to modify this part\n",
    "\n",
    "# These are the *actual* values of C0 ... C4 we're trying to estimate.\n",
    "# Shh... They're a secret! :) This is what we're trying to estimate.\n",
    "bagtype_count = 5\n",
    "actual_cs = (0.2, 0.3, 0.7, 0.9, 1.0)\n",
    "\n",
    "# Now sample 100 bags\n",
    "bag_count = 100\n",
    "actual_bagtypes = [ random.randrange(0, bagtype_count) for _ in range(0, bag_count) ]\n",
    "\n",
    "# Now sample 100 candies from each bag, to produce a list-of-lists\n",
    "# The result is a list of bag_count lists. Each list has candy_count members that are either \"X\" or \"C\", \n",
    "# drawn randomly according to a distribution\n",
    "candy_count = 100\n",
    "observations = [ [ (\"L\", \"C\")[x] for x in tuple(np.random.binomial( 1, actual_cs[ bagtype ], candy_count ) ) ] \n",
    "                 for bagtype in actual_bagtypes ] \n",
    "\n",
    "# This list will hold your estimated C0 ... C4 values, and your estimated\n",
    "# bagtype for each bag.\n",
    "estimated_cs = [0] * bagtype_count\n",
    "estimated_bagtypes = np.array([0] * bag_count) # cfalter: changed from list to ndarray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Algorithm\n",
    "The algorithm iteratively performs two steps until the estimates of the latent parameters converge:\n",
    "**1. Estimation (E)**: Given a set of labels for the latent parameters, estimate (assign) the label corresponding to the most likely parameter. In our scenario, the labels correspond to probabilities used by the manufacturer when packaging each of the bags.\n",
    "**2. Maximization (M)**: For each label, estimate the parameter value that maximizes the likelihood of its corresponding observations. In our scenario, the parameter values is a probability of drawing cherry candies, and we calculate it as the mean of the distributions assigned to a label.\n",
    "\n",
    "#### Initialization: Chicken or egg? A random answer\n",
    "You need a set of parameter values to perform the Estimation step that assigns labels; but you need labeled distributions to perform the Maximization step. Since each step depends on the output of the other, how can we start the algorithm? The answer is simple: you initialize by assigning random parameter values. \n",
    "\n",
    "#### How likely is the result? \n",
    "The likelihood of each possible label for each bag is calculated in the Estimation step: The maximum likelihood indicates the best label. Dividing the highest parameter likelihood by the sum of all parameter likelihoods yields the likelihood that the label represents the ground truth. We can multiply these likelihoods together to find the likelihood that a given set of parameters is the ground truth. This cumulative likelihood increases with each iteration of the algorithm and reaches a plateau when the algorithm converges.\n",
    "\n",
    "Because this likelihood can be quite low (as low as 20%) and we have 100 observations, it is better to sum the log of the likelihoods as opposed to multiplying the likelihoods. This relies on the logarithm product rule: `log(ab) = log(a) + log(b)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "\n",
    "C_counts = np.array([np.sum(np.array(observations[i]) == 'C') for i in range(bag_count)])\n",
    "\n",
    "def em(counts):\n",
    "    # E-M parameters\n",
    "    max_iters = 100 # if E-M algorithm hasn't converged after this many iterations, just stop\n",
    "    bagtypes = np.array([0] * bag_count)\n",
    "    \n",
    "    # randomly initialize estimates of c (probability of drawing cherry candy from a bag)\n",
    "    cs = np.random.uniform(0.001, 1.0, bagtype_count)\n",
    "    old = np.copy(cs)\n",
    "    \n",
    "    # run EM\n",
    "    for i in range(max_iters):\n",
    "        loglikelihood = 0.0\n",
    "        likelihoodFns = [ss.binom(candy_count, c) for c in cs]\n",
    "        \n",
    "        # E-step\n",
    "        for bag in range(bag_count):\n",
    "            C = counts[bag]\n",
    "            likelihoods = np.array([likelihoodFns[j].pmf(C) for j in range(bagtype_count)])\n",
    "            most_likely = np.argmax(likelihoods)\n",
    "            bagtypes[bag] = most_likely\n",
    "            loglikelihood_b = math.log(likelihoods[most_likely]/np.sum(likelihoods))\n",
    "            loglikelihood += loglikelihood_b\n",
    "        \n",
    "        # M-step\n",
    "        for j in range(bagtype_count):\n",
    "            if np.sum(bagtypes == j) == 0:\n",
    "                # a bad initialization resulted in an estimated_c that was never most likely\n",
    "                # for any of the bags. Do a random restart for that estimated_c\n",
    "                cs[j] = random.uniform(0.001, 1.0)\n",
    "            else:\n",
    "                cs[j] = np.mean(counts[bagtypes == j]) / float(candy_count)\n",
    "            \n",
    "        if np.all(old == cs):\n",
    "            break\n",
    "        \n",
    "        old = np.copy(cs)\n",
    "        \n",
    "    return loglikelihood, cs, bagtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem of Local Maxima\n",
    "The EM algorithm often converges to a very good approximation of the latent parameters. Unfortunately, this happy result is, like most happy results in life, not guaranteed. A quirky random initialization of the latent parameters can cause the algorithm to converge to a local maximum likelihood that is not the global maximum likelihood. For example, a fairly frequent parameter estimation in the cherry candy scenario was the set {0.3, 0.3, 0.66, 0.74, 0.95}--compared to the ground truth of {0.2, 0.3, 0.7, 0.9, 1.0}. Therefore I ran the EM algorithm 25 times and selected the one output of the 25 that maximized the likelihood of the observations. If I recall correctly, the probability that the algorithm would reach a local rather than a global maximum in this scenario was about 1/4. Thus the probability that the global maximum will not be found by running the algorithm 25 times is (1/4)<sup>25</sup>, or about 1 in 1,125,899,910,000,000. In a word, infinitesimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_Ems = 25\n",
    "results = []\n",
    "for i in range(num_Ems):\n",
    "    likelihood, cs, bagtypes = em(C_counts)\n",
    "    results.append((likelihood, cs, bagtypes))\n",
    "\n",
    "# sort on likelihood and select the winner    \n",
    "winner = sorted(results, key = lambda x: x[0], reverse = True)[0]\n",
    "estimated_cs = winner[1]\n",
    "estimated_bagtypes = winner[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print and You're Done\n",
    "The remainder of the code was provided by Professor Crandall so he and the TAs could grade the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual C's:          (0.2, 0.3, 0.7, 0.9, 1.0)\n",
      "Estimated C's:       [0.16250000000000001, 0.27740740740740738, 0.71400000000000008, 0.89631578947368429, 1.0]\n",
      "Actual bagtypes:     [2, 3, 1, 3, 1, 1, 0, 3, 0, 3, 4, 4, 1, 0, 3, 1, 3, 0, 3, 0, 1, 3, 2, 2, 4, 4, 1, 4, 1, 1, 3, 4, 1, 1, 4, 0, 2, 2, 2, 4, 4, 1, 4, 3, 4, 2, 0, 4, 2, 0, 0, 3, 1, 4, 0, 0, 0, 2, 3, 1, 1, 2, 3, 1, 4, 0, 3, 2, 2, 3, 3, 4, 2, 1, 2, 4, 0, 4, 4, 4, 0, 3, 2, 1, 2, 1, 0, 2, 3, 4, 3, 2, 2, 2, 1, 0, 4, 0, 0, 4]\n",
      "Estimated bagtypes:  [2, 3, 1, 3, 1, 1, 0, 3, 1, 3, 4, 4, 1, 0, 3, 1, 3, 0, 3, 1, 1, 3, 2, 2, 4, 4, 1, 4, 1, 1, 3, 4, 1, 1, 4, 0, 2, 2, 2, 4, 4, 1, 4, 3, 4, 2, 0, 4, 2, 0, 1, 3, 1, 4, 0, 0, 1, 2, 3, 1, 1, 2, 3, 1, 4, 1, 3, 2, 2, 3, 3, 4, 2, 1, 2, 4, 0, 4, 4, 4, 1, 3, 2, 1, 2, 1, 0, 2, 3, 4, 3, 2, 2, 2, 1, 1, 4, 0, 0, 4]\n",
      "Correctly estimated bags:  93\n"
     ]
    }
   ],
   "source": [
    "sorted_cs = sorted((e,i) for i,e in enumerate(estimated_cs))\n",
    "estimated_cs = [ v[0] for v in sorted_cs ]\n",
    "index_remapper = [ 0 ] * bagtype_count\n",
    "for i in range(0, bagtype_count):\n",
    "    index_remapper[ sorted_cs[i][1] ] = i\n",
    "estimated_bagtypes = [ index_remapper[bagtype] for bagtype in estimated_bagtypes ]\n",
    "\n",
    "print (\"Actual C's:         \", actual_cs)\n",
    "print (\"Estimated C's:      \", estimated_cs)\n",
    "print (\"Actual bagtypes:    \", actual_bagtypes)\n",
    "print (\"Estimated bagtypes: \", estimated_bagtypes)\n",
    "print (\"Correctly estimated bags: \", sum( [ actual_bagtypes[i] == estimated_bagtypes[i] for i in range(0, len(estimated_bagtypes) ) ] ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
