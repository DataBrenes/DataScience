Artificial intelligence is the hottest of the hot new buzzwords, but for a very good reason--it is yielding remarkable advances in a variety of fields. Consigned to the dustbin of computer science in the 1990s, it re-emerged in about 2012 with a vengeance when a combination of new hardware, new methods, and massive datasets yielded fantastic results. Artificial intelligence can now identify the digits in a zip code or the faces in a picture more reliably than humans. 

If you really want to succeed in the field, however, you must understand its foundations. Professor David Crandall brought out the best in all of us grad students in his Fall 2017 course on the Elements of Artificial Intelligence. In this section of my portfolio, I publish some of the most useful things I learned and most useful code I wrote.

+ [Expectation Maximization Algorithm](https://github.com/chrisfalter/DataScience/blob/master/AI/Expectation_Maximization.ipynb) - Expectation Maximization (EM) is an algorithm you can use to estimate the maximum likelihood for the value of latent (hidden) parameters. Suppose, for example, that your friend possesses a bag of five unfair coins, but you don't know the "weights" of the coins--i.e., each coin's probability of coming up heads. Your friend offers to shake the bag, pick a coin out of the bag, flip it 100 times, then put it back in the bag. He goes through this cycle this 100 times. The visible parameters are the 100 sessions of heads or tails; the latent parameters are the "weights" of the five coins. How can you estimate the latent parameters? EM provides a useful method.
+ [Search](https://github.com/chrisfalter/DataScience/tree/master/AI/Search) - Most software developers know something about search because of their familiarity with databases, which facilitate searching for IDs or values by storing search keys in data structures such as binary trees and employing algorithms such as binary search. Artificial intelligence expands the use case for search algorithms by using them in games (find the best chess move), map applications (find the fastest route to Clarksville), and robotics, among other things. Machine learning is a key application for search, as well; gradient descent to minimize a loss function can be regarded as a form of local search over the solution space of all parameter combinations. In fact, some of the most exciting research in neural networks borrows local search techniques; [stochastic gradient descent with warm restarts](https://arxiv.org/abs/1608.03983), for example, can be regarded as a form of the search algorithm known as hill climbing with restarts.
