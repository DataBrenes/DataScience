Artificial intelligence is the hottest of the hot new buzzwords, but for a very good reason--it is yielding remarkable advances in a variety of fields. Consigned to the dustbin of computer science in the 1990s, it re-emerged in about 2012 with a vengeance when a combination of new hardware, new methods, and massive datasets yielded fantastic results. Artificial intelligence can now identify the digits in a zip code or the faces in a picture more reliably than humans. 

If you really want to succeed in the field, however, you must understand its foundations. Professor David Crandall brought out the best in all of us grad students in his Fall 2017 course on the Elements of Artificial Intelligence. In this section of my portfolio, I publish some of the most useful things I learned and most useful code I wrote.

+ [Expection Maximization Algorithm](https://github.com/chrisfalter/DataScience/blob/master/AI/Expectation_Maximization.ipynb) - Expectation Maximization (EM) is an algorithm you can use to estimate the maximum likelihood for the value of latent (hidden) parameters. Suppose, for example, that your friend possesses a bag of five unfair coins, but you don't know the "weights" of the coins--i.e., each coin's probability of coming up heads. Your friend offers to shake the bag, pick a coin out of the bag, flip it 100 times, then put it back in the bag. He goes through this cycle this 100 times. The visible parameters are the 100 sessions of heads or tails; the latent parameters are the "weights" of the five coins. How can you estimate the latent parameters? EM provides a useful method.
