# Machine Learning for Big Data with Apache Spark
Python machine learning algorithms work well on single machines, but typically do not scale across multiple machines. You can find some libraries for specific topics (e.g., `gensim` for NLP) that do scale horizontally; however, each library has its own scaling API. 

Into this chaos steps Apache Spark, which provides excellent horizontal scaling together with standardized feature handlers, data types, and models for a wide variety of machine learning tasks. It is built on the map/reduce paradigm founded by Apache Hadoop, but optimizes quite capably for iterative algorithms common to machine learning. Spark algorithms [often run about an order of magnitude faster](https://en.wikipedia.org/wiki/Apache_Spark) than comparable Map-Reduce algorithms. If you need to build models and stream predictions for big data applications, and you want the ability to code for yourself (as opposed to adopting a vendor's "low code" dashboard), there is no better option than Spark.
