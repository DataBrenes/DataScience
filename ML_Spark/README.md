# Machine Learning for Big Data with Apache Spark
Python machine learning algorithms work well on single machines, but typically do not scale across multiple machines. You can find some libraries for specific topics (e.g., `gensim` for NLP) that do scale horizontally; however, each library has its own scaling API. 

Into this chaos steps Apache Spark, which provides excellent horizontal scaling together with standardized feature handlers, data types, and models for a wide variety of machine learning tasks. It is built on the map/reduce paradigm founded by Apache Hadoop, but optimizes quite capably for iterative algorithms common to machine learning. Spark algorithms [often run about an order of magnitude faster](https://en.wikipedia.org/wiki/Apache_Spark) than comparable Map-Reduce algorithms. If you need to build models and stream predictions for big data applications, and you want the ability to code for yourself (as opposed to adopting a vendor's "low code" dashboard), there is no better option than Spark.

## Explorations
+ [Analyzing White House Visits](./Top_Whitehouse_Visitors_SPARK.ipynb) - Who has the most access to the President? If you had to sort through a half-million records by hand, it would be very hard work. This exploration shows how to build a resilient distributed dataset (RDD) and transform the data through mapping, filtering, reducing, and sorting operations. RDDs are in-memory structures that span multiple nodes in a Spark cluster, and knowing how to use them unlocks the power of Spark.
+ [Spam Classification with PySpark](./SpamClassifier_SPARK.ipynb) - This notebook builds a spam classifier against the CSMDC 2010 Spam data set, which has 4327 labeled observations. It uses the PySpark interface to Spark MLLib, which provides a lot of useful functionality. However, the exploration runs into some serious shortcomings in `mllib` for real-world use. Fortunately, Spark also provides updated machine learning functionality in the `'ml` library, which future notebooks will explore. Pinkie promise!
