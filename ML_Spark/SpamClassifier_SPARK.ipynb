{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Spam Classifier with PySpark\n",
    "This adventure builds a spam classifier against the CSMDC 2010 Spam data set, which has 4327 labeled observations. While the original dataset has disappeared from its original home, you may download it from [this Github repo](https://github.com/hexgnu/spam_filter/tree/master/data).\n",
    "\n",
    "This exercise is loosely based on a lab from the Hortonworks *HDP Analyst: Data Science* course.\n",
    "\n",
    "We will read the files with standard Python code, then create a Resilient Distributed Dataset from the data that have been loaded into the driver's memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spamFiles = []\n",
    "notSpamFiles = [] \n",
    "\n",
    "with open('/root/ds/labs/Lab10.3/spamClassData/label/SPAMTrain.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        if int(line[0]) == 1:\n",
    "            r = line[2:]\n",
    "            notSpamFiles.append('/root/ds/labs/Lab10.3/spamClassData/'+r.rstrip('\\n'))\n",
    "        elif int(line[0]) == 0:\n",
    "            r = line[2:]\n",
    "            spamFiles.append('/root/ds/labs/Lab10.3/spamClassData/'+r.rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def loadFiles(paths):\n",
    "    emails = []\n",
    "    for path in paths:\n",
    "        with io.open(path,\"r\", encoding='iso-8859-1') as f:\n",
    "            emails.append(f.read())\n",
    "    return emails\n",
    "\n",
    "spams = loadFiles(spamFiles)\n",
    "hams = loadFiles(notSpamFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline\n",
    "This pipeline is built with code I previously published. I should really turn this into a library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "1378\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from nltk import download, pos_tag\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import string\n",
    "import re\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class TransformerBase(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Provides no-op fit() function for Transformers that only need\n",
    "    a fit method\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "class LowerCaser(TransformerBase):\n",
    "    \n",
    "    def transform(self, X, **fit_params):\n",
    "        for i in range(len(X)):\n",
    "            X[i] = X[i].lower()\n",
    "        return X    \n",
    "\n",
    "class Tokenizer(TransformerBase):\n",
    "\n",
    "    def transform(self, X, **fit_params):\n",
    "        for i in range(len(X)):\n",
    "            X[i] = X[i]\n",
    "            X[i] = wordpunct_tokenize(X[i])\n",
    "            for tok in X[i]:\n",
    "                if tok.endswith('.') and len(tok) > 1:\n",
    "                    X[i].remove(tok)\n",
    "        return X\n",
    "    \n",
    "def remove_listed_tokens(X, removal_list):\n",
    "    '''\n",
    "    If you immediately remove a token as you iterate forward through a list,\n",
    "    you skip over the next token. This function instead builds a list of tokens\n",
    "    to be removed, then removes them at the end.\n",
    "    \n",
    "    Parameters\n",
    "      X - list of lists\n",
    "      removal_list - string of tokens (e.g., punctuation), or list of strings (e.g., stopwords)\n",
    "    '''\n",
    "    for doc in X:\n",
    "        removals = []\n",
    "        for tok in doc:\n",
    "            if tok in removal_list:\n",
    "                removals.append(tok)\n",
    "        for p in removals:\n",
    "            doc.remove(p)\n",
    "    return X\n",
    "        \n",
    "class StopWordRemover(TransformerBase):\n",
    "    \n",
    "    def __init__(self):\n",
    "        download(\"stopwords\")\n",
    "        \n",
    "    def transform(self, X, **fit_params):\n",
    "        return remove_listed_tokens(X, stopwords.words('english'))\n",
    "    \n",
    "class Stringizer(TransformerBase):\n",
    "    def transform(self, X, **fit_params):\n",
    "        for i in range(len(X)):\n",
    "            X[i] = ' '.join(X[i])\n",
    "        return X\n",
    "    \n",
    "class Lemmatizer(TransformerBase):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.treenet_map = defaultdict(str)\n",
    "        self.treenet_map['N'] = wordnet.NOUN\n",
    "        self.treenet_map['R'] = wordnet.ADV\n",
    "        self.treenet_map['V'] = wordnet.VERB\n",
    "        self.treenet_map['J'] = wordnet.ADJ\n",
    "        \n",
    "    def transform(self, X, **fit_params):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        for i in range(len(X)):\n",
    "            doc = X[i][:]\n",
    "            X[i] = [] # a list of lemmatized tokens\n",
    "            for tok, pos in pos_tag(doc):\n",
    "                wordnet_pos = self.treenet_map[pos[0]]\n",
    "                if not wordnet_pos:\n",
    "                    X[i].append(tok) # use tok without any lemmatizing if not a recognized POS\n",
    "                else:\n",
    "                    X[i].append(lemmatizer.lemmatize(tok, wordnet_pos))\n",
    "        return X\n",
    "\n",
    "class PunctuationRemover(TransformerBase):\n",
    "    \n",
    "    def __init__(self, exceptions = ''):\n",
    "        self.exceptions = exceptions\n",
    "        \n",
    "    def transform(self, X, **fit_params):\n",
    "        if not self.exceptions:\n",
    "            punc = string.punctuation\n",
    "        else:\n",
    "            retained_punc = re.compile('['+self.exceptions+']') # don't remove these chars; they may convey emotion\n",
    "            punc = retained_punc.sub('', string.punctuation)\n",
    "        return remove_listed_tokens(X, punc)\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {'exceptions': self.exceptions}\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parm, value in parameters.items():\n",
    "            setattr(self, parm, value)\n",
    "        return self\n",
    "\n",
    "pipeline = Pipeline([('lower', LowerCaser()),\n",
    "                     ('tokenize', Tokenizer()),\n",
    "                     ('lemmatize', Lemmatizer()),\n",
    "                     ('stopwords', StopWordRemover()),\n",
    "                     ('punc', PunctuationRemover()),\n",
    "                     ('stringize', Stringizer())])\n",
    "\n",
    "spams = pipeline.transform(spams)\n",
    "hams = pipeline.transform(hams)\n",
    "print len(spams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark MLLib\n",
    "Because this notebook was created and run on a vastly underpowered, single-node cluster, the SparkConf must throttle the resource usage. The string data will be:\n",
    "1. parallelized into RDDs\n",
    "2. tokenized into a sparse feature vector with `pyspark.mllib.feature.HashingTF`\n",
    "\n",
    "Since a model from `pyspark.mllib` will process the data, we must use the `mllib` version of `HashingTF` rather than the `pyspark.ml` version.\n",
    "\n",
    "We will then prepend the labels to the observations so the LogisticRegressionWithSGD classifier can train. After splitting the observations into training and test sets, we cache the training set to optimize the iterative stochastic gradient descent processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().set('spark.executor.instances', 1). \\\n",
    " set(\"spark.executor.memory\", \"4g\")\n",
    "sc = SparkContext(\"yarn-client\", conf=conf)\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "\n",
    "spam = sc.parallelize(spams)\n",
    "ham = sc.parallelize(hams)\n",
    "tf = HashingTF()\n",
    "\n",
    "spamFeatures = spam.map(lambda email: tf.transform(email.split(\" \")))\n",
    "hamFeatures = ham.map(lambda email: tf.transform(email.split(\" \")))\n",
    "\n",
    "labeledSpam = spamFeatures.map(lambda features: LabeledPoint(1, features))\n",
    "labeledHam = hamFeatures.map(lambda features: LabeledPoint(0, features))\n",
    "\n",
    "observations = labeledSpam.union(labeledHam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingSet, testSet = observations.randomSplit([4,1])\n",
    "trainingSet.cache()\n",
    "model = LogisticRegressionWithSGD.train(trainingSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation\n",
    "Let's test how well the model performs...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.026875699888\n"
     ]
    }
   ],
   "source": [
    "LabelsAndPredictions = testSet.map(lambda v: (v.label, model.predict(v.features)))\n",
    "LabelsAndPredictions.cache()\n",
    "errorRate = LabelsAndPredictions.filter(lambda lp: lp[0] != lp[1]).count() / float(LabelsAndPredictions.count())\n",
    "print(\"Training Error = \" + str(errorRate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, `pyspark.mllib.feature.HashingTF` does not provide a way to inspect the terms in the sparse feature vector. If you want to understand feature importance, you must use `pyspark.ml.feature.CountVectorizer` with a model from the `pyspark.ml` namespace. Since `pyspark.mllib` models such as `LogisticRegressionWithSGD` cannot even be saved to disk at this time, the only reasonable way to use PySpark in production data science is to use `pyspark.ml` rather than `pyspark.mllib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
