{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc_conf = SparkConf() \\\n",
    "    .setAppName('ProductData') \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .set('spark.executor.cores', '3') \\\n",
    "    .set('spark.executor.memory', '1536m') \\\n",
    "    .set('spark.driver.cores', '1') \\\n",
    "    .set('spark.driver.memory', '1536m') \\\n",
    "    .set('spark.python.worker.memory', '1g')\n",
    "sc = SparkContext(conf=sc_conf)\n",
    "spark = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+-------------------+-----+\n",
      "|        id|               title|         description|       manufacturer|price|\n",
      "+----------+--------------------+--------------------+-------------------+-----+\n",
      "|b000jz4hqo|clickart 950 000 ...|                null|         broderbund|  0.0|\n",
      "|b0006zf55o|ca international ...|oem arcserve back...|computer associates|  0.0|\n",
      "+----------+--------------------+--------------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PART 1: Read each file and create RDD consisting of lines\n",
    "google_file = \"./data/Google.csv\"\n",
    "google_df = spark.read.csv(google_file, header = True, inferSchema = True)\n",
    "amazon_file = \"./data/Amazon.csv\"\n",
    "amazon_df = spark.read.csv(amazon_file, header = True, inferSchema = True)\n",
    "amazon_df.show(n = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+-------------------+-----+--------------------+\n",
      "|        id|               title|         description|       manufacturer|price|              tokens|\n",
      "+----------+--------------------+--------------------+-------------------+-----+--------------------+\n",
      "|b000jz4hqo|clickart 950 000 ...|                    |         broderbund|  0.0|                  []|\n",
      "|b0006zf55o|ca international ...|oem arcserve back...|computer associates|  0.0|[oem, arcserve, b...|\n",
      "+----------+--------------------+--------------------+-------------------+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PART 2: Remove stopwords, tokenize with regex\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "\n",
    "# substitute empty string for NULL so transforms don't fail\n",
    "amazon_df = amazon_df.fillna('')\n",
    "google_df = google_df.fillna('')\n",
    "\n",
    "# tokenize the description strings, separating on whitespace\n",
    "regex = RegexTokenizer(inputCol = \"description\", outputCol = \"tokens\", pattern=\"\\\\W\")\n",
    "amz_tokens = regex.transform(amazon_df)\n",
    "ggl_tokens = regex.transform(google_df)\n",
    "amz_tokens.show(n = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+-------------------+-----+--------------------+--------------------+\n",
      "|        id|               title|         description|       manufacturer|price|              tokens|            filtered|\n",
      "+----------+--------------------+--------------------+-------------------+-----+--------------------+--------------------+\n",
      "|b000jz4hqo|clickart 950 000 ...|                    |         broderbund|  0.0|                  []|                  []|\n",
      "|b0006zf55o|ca international ...|oem arcserve back...|computer associates|  0.0|[oem, arcserve, b...|[oem, arcserve, b...|\n",
      "+----------+--------------------+--------------------+-------------------+-----+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# REMOVE STOP WORDS\n",
    "# get the stopwords list - this is horribly inefficient, but it's the only way I can get Spark\n",
    "# to pick up the file from the Databricks environment\n",
    "sw_file = \"./data/stopwords.txt\"\n",
    "stopwords_rdd = sc.parallelize(sw_file)\n",
    "stopwords_list = stopwords_rdd.collect()\n",
    "# create and use the stop words filter\n",
    "stopword_filter = StopWordsRemover(inputCol = \"tokens\", outputCol = \"filtered\", stopWords = stopwords_list)\n",
    "ggl_filtered = stopword_filter.transform(ggl_tokens)\n",
    "amz_filtered = stopword_filter.transform(amz_tokens)\n",
    "amz_filtered.show(n = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 3: Write a function that takes a list of tokens and returns a dictionary mapping tokens to term frequency\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def make_term_freq_dict(tokens, normalize = True):\n",
    "  '''\n",
    "  Returns a dictionary mapping term in a collection of tokens to its *normalized* frequency\n",
    "  '''\n",
    "  d = {}\n",
    "  if normalize:\n",
    "    normalization_factor = len(tokens)\n",
    "  else:\n",
    "    normalization_factor = 1.0\n",
    "  for tok in tokens:\n",
    "    if not tok in d:\n",
    "      d[tok] = 1./normalization_factor\n",
    "    else:d[tok] += 1./normalization_factor\n",
    "  return d   \n",
    "\n",
    "def term_freq(df, tokensColumn, mapColumn, normalize = True):\n",
    "  '''\n",
    "  Returns a Spark dataframe with a map of token to normalized count in the (new) designated column.\n",
    "  \n",
    "  Parameters:\n",
    "  df - pyspark.DataFrame instance\n",
    "  tokensColumn - name of column containing the tokens to be counted\n",
    "  mapColumn - name of column to which normalized term-frequency map will be written for each row\n",
    "  '''\n",
    "  newColumnList = df.columns + [mapColumn]\n",
    "  new_rdd = df.rdd.map(lambda row: (row + Row(make_term_freq_dict(row[tokensColumn], normalize))))\n",
    "  return new_rdd.toDF(newColumnList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                      tf|\n",
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                      []|\n",
      "|[1 -> 1.0, arcserve -> 1.0, backup -> 1.0, desktops -> 1.0, and -> 1.0, oem -> 1.0, v11 -> 1.0, for -> 1.0, 30u -> 1....|\n",
      "|                                                                                                                      []|\n",
      "|[year -> 1.0, advanced -> 1.0, striving -> 1.0, backed -> 1.0, costing -> 1.0, convert -> 1.0, your -> 5.0, billing -...|\n",
      "|[singing -> 1.0, nt -> 1.0, unlimited -> 1.0, xp -> 1.0, me -> 1.0, electronic -> 1.0, 2000 -> 1.0, learning -> 1.0, ...|\n",
      "|                     [disk -> 1.0, diskcromwindows -> 1.0, 5 -> 1.0, 7 -> 1.0, emc -> 1.0, to -> 1.0, retrospect -> 1.0]|\n",
      "|[standard -> 1.0, interactive -> 1.0, your -> 1.0, caching -> 1.0, output -> 1.0, preserved -> 1.0, tighter -> 1.0, l...|\n",
      "|[naturallyspeaking -> 3.0, temporary -> 1.0, accurate -> 1.0, software -> 2.0, safer -> 1.0, accuracy -> 1.0, corel -...|\n",
      "|[using -> 1.0, mia -> 2.0, will -> 1.0, in -> 2.0, save -> 1.0, their -> 1.0, house -> 1.0, skills -> 1.0, adventure ...|\n",
      "|[love -> 1.0, education -> 1.0, keys -> 1.0, for -> 1.0, their -> 1.0, learning -> 1.0, secret -> 1.0, pixar -> 1.0, ...|\n",
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test term_freq function\n",
    "counted = term_freq(amz_filtered, \"filtered\", \"tf\", False)\n",
    "counted.select('tf').show(n = 10, truncate = 120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 4: combine the two filtered dataframes into a single \"corpus\"\n",
    "combined = ggl_filtered.select('id', 'filtered').union(amz_filtered.select('id','filtered'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 5: An IDF function that returns a pair RDD having key = token and value = IDF. \n",
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "\n",
    "def token_idf_pair_rdd(df, tokenCol):\n",
    "  '''\n",
    "  Calculates IDF for all tokens in designated column; \n",
    "  returns a pair RDD with token as key and IDF as value\n",
    "  '''\n",
    "  # The counter is binary per document-word because that is the simplest method of calculation\n",
    "  counter = CountVectorizer(binary = True, inputCol = tokenCol, outputCol = 'counted')\n",
    "  count_model = counter.fit(df)\n",
    "  counted = count_model.transform(df)\n",
    "  idfer = IDF(inputCol = 'counted', outputCol = 'idf')\n",
    "  idf_model = idfer.fit(counted)\n",
    "  tf_idf = idf_model.transform(counted)\n",
    "  data = [(count_model.vocabulary[i], float(idf_model.idf[i])) for i in range(len(count_model.vocabulary))]\n",
    "  return sc.parallelize(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 0.47972392120040291),\n",
       " ('the', 0.57338112086856174),\n",
       " ('to', 0.71606576853438264),\n",
       " ('of', 0.8653242882821276),\n",
       " ('your', 1.0658224658451187)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that the function works\n",
    "idf_map_rdd = token_idf_pair_rdd(combined, 'filtered')\n",
    "idf_map_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f3aae04a0f0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram of IDF values\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "idf_hist = idf_map_rdd.map(lambda x: x[1]).histogram(10)\n",
    "idf_hist_list = list(zip(*idf_hist))\n",
    "pd_hist = pd.DataFrame(idf_hist_list, columns = [\"idf\", \"count\"])\n",
    "pd_hist.set_index('idf')\n",
    "pd_hist.plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 6: Write a function which does the following on entire corpus:\n",
    "# (a) Calculate term frequencies (not normalized) for tokens\n",
    "# (b) Create a DataFrame where each token maps to the token's frequency times the token's IDF weight\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import StructField, StructType, DoubleType, StringType\n",
    "\n",
    "def word_count_df(df, tokenCol, wordColumnName = 'word', countColumnName = 'count'):\n",
    "  '''\n",
    "  Returns a dataframe with 2 columns:\n",
    "    * keyColumnName - tokens\n",
    "    * valueColumnName - count of token aggregated over all rows in df\n",
    "  '''\n",
    "  # Note that the counts are *not* normalized per-document\n",
    "  df_mapped = term_freq(df, tokenCol, 'tf', normalize = False)\n",
    "  df_word_count = df_mapped.select(func.explode('tf'))\n",
    "  df_word_count = df_word_count.groupby('key').agg(func.sum('value').alias(countColumnName))\n",
    "  return df_word_count.withColumnRenamed('key', wordColumnName)\n",
    "  \n",
    "def tf_idf_over_corpus(df, tokenCol):\n",
    "  '''\n",
    "  Returns a dataframe with two columns:\n",
    "  * \"word\": string - unique token\n",
    "  * \"tfidf\": float - count-in-corpus times idf\n",
    "  Also returns a pair RDD with 1st column/key = token and 2d col/value = IDF \n",
    "  \n",
    "  Parameters:\n",
    "  df - DataFrame\n",
    "  tokenCol:string - name of column containing tokens\n",
    "  '''\n",
    "  # drop all the columns except for the tokens column\n",
    "  columnList = df.columns\n",
    "  columnList.remove(tokenCol)\n",
    "  for c in columnList:\n",
    "    df = df.drop(c)\n",
    "    \n",
    "  # get an idf pair rdd (word -> idf)\n",
    "  idf_rdd = token_idf_pair_rdd(df, tokenCol)\n",
    "  df_idf_schema = StructType([\n",
    "    StructField(\"word\", StringType()),\n",
    "    StructField(\"idf\", DoubleType())\n",
    "  ])\n",
    "  df_idf = spark.createDataFrame(idf_rdd, df_idf_schema)\n",
    "  \n",
    "  # get a dataframe with two columns, 'word' and 'count'\n",
    "  df_wc = word_count_df(df, tokenCol, wordColumnName = 'word', countColumnName = 'count')\n",
    "  df_wc.drop(tokenCol)\n",
    "  \n",
    "  # join on the 'word' column\n",
    "  df_wc_idf = df_wc.join(df_idf, on = 'word')\n",
    "  \n",
    "  # do the multiplication to yield the tf-idf\n",
    "  return df_wc_idf.withColumn('tfidf', func.col('count') * func.col('idf')).drop('count').drop('idf'), idf_rdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(word,StringType,false),StructField(tfidf,DoubleType,true)))\n",
      "['word', 'tfidf']\n",
      "StructType(List(StructField(map,MapType(StringType,DoubleType,true),false)))\n",
      "['map']\n"
     ]
    }
   ],
   "source": [
    "# test the tf_idf_over_corpus function, which returns \n",
    "# (1) a DF with columns 'word' and 'tfidf'\n",
    "# (2) a pair RDD with key = token and value = IDF\n",
    "\n",
    "from itertools import chain\n",
    "from pyspark.sql.functions import col, create_map\n",
    "\n",
    "df_tfidf, _ = tf_idf_over_corpus(combined, 'filtered')\n",
    "\n",
    "# Let's transform the data to a mapped format, with a single column that maps word->tfidf\n",
    "map_column = create_map(list(chain((col('word'), col('tfidf'))))).alias('map')\n",
    "df_map = df_tfidf.select(map_column)\n",
    "\n",
    "# Let's take a peek at the map dataframe\n",
    "print(df_tfidf.schema)\n",
    "print(df_tfidf.columns)\n",
    "print(df_map.schema)\n",
    "print(df_map.columns)\n",
    "\n",
    "# Save to a file in json format \n",
    "dfmap_file = \"./data/d.json\"\n",
    "df_map.write.json(dfmap_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
